// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: query.sql

package db

import (
	"context"

	"github.com/jackc/pgx/v5/pgtype"
)

const brokenIndexes = `-- name: BrokenIndexes :many
SELECT
  tblclass.relname AS table_name
  , idxclass.relname AS index_name
FROM pg_index
INNER JOIN pg_class AS idxclass ON pg_index.indexrelid = idxclass.oid
INNER JOIN pg_class AS tblclass ON pg_index.indrelid = tblclass.oid
WHERE NOT pg_index.indisvalid
`

type BrokenIndexesRow struct {
	TableName string
	IndexName string
}

func (q *Queries) BrokenIndexes(ctx context.Context) ([]BrokenIndexesRow, error) {
	rows, err := q.db.Query(ctx, brokenIndexes)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []BrokenIndexesRow
	for rows.Next() {
		var i BrokenIndexesRow
		if err := rows.Scan(&i.TableName, &i.IndexName); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const connectionStats = `-- name: ConnectionStats :one
SELECT
  current_setting('max_connections')::int AS max_connections
  , current_setting('superuser_reserved_connections')::int AS reserved_connections
  , count(*) AS total_connections
  , count(*) FILTER (WHERE state = 'active') AS active_connections
  , count(*) FILTER (WHERE state = 'idle') AS idle_connections
  , count(*) FILTER (WHERE state = 'idle in transaction') AS idle_in_transaction
  , count(*) FILTER (WHERE state = 'idle in transaction (aborted)') AS idle_in_transaction_aborted
  , count(*) FILTER (WHERE wait_event_type IS NOT NULL AND state = 'active') AS waiting_connections
FROM pg_stat_activity
WHERE pid != pg_backend_pid()
`

type ConnectionStatsRow struct {
	MaxConnections           pgtype.Int4
	ReservedConnections      pgtype.Int4
	TotalConnections         pgtype.Int8
	ActiveConnections        pgtype.Int8
	IdleConnections          pgtype.Int8
	IdleInTransaction        pgtype.Int8
	IdleInTransactionAborted pgtype.Int8
	WaitingConnections       pgtype.Int8
}

// Gets overall connection statistics including pool sizing metrics.
func (q *Queries) ConnectionStats(ctx context.Context) (ConnectionStatsRow, error) {
	row := q.db.QueryRow(ctx, connectionStats)
	var i ConnectionStatsRow
	err := row.Scan(
		&i.MaxConnections,
		&i.ReservedConnections,
		&i.TotalConnections,
		&i.ActiveConnections,
		&i.IdleConnections,
		&i.IdleInTransaction,
		&i.IdleInTransactionAborted,
		&i.WaitingConnections,
	)
	return i, err
}

const databaseCacheEfficiency = `-- name: DatabaseCacheEfficiency :one
SELECT
  blks_hit
  , blks_read
  , stats_reset
  , CASE
    WHEN blks_hit + blks_read = 0 THEN NULL
    ELSE round(100.0 * blks_hit / (blks_hit + blks_read), 2)
  END AS cache_hit_ratio
  , coalesce(
    extract(EPOCH FROM (now() - stats_reset)) / 86400
    , 999
  ) AS stats_age_days
FROM pg_stat_database
WHERE datname = current_database()
`

type DatabaseCacheEfficiencyRow struct {
	BlksHit       pgtype.Int8
	BlksRead      pgtype.Int8
	StatsReset    pgtype.Timestamptz
	CacheHitRatio pgtype.Numeric
	StatsAgeDays  pgtype.Numeric
}

// Returns database-wide buffer cache hit ratio.
// Low ratios indicate shared_buffers too small or working set exceeds memory.
func (q *Queries) DatabaseCacheEfficiency(ctx context.Context) (DatabaseCacheEfficiencyRow, error) {
	row := q.db.QueryRow(ctx, databaseCacheEfficiency)
	var i DatabaseCacheEfficiencyRow
	err := row.Scan(
		&i.BlksHit,
		&i.BlksRead,
		&i.StatsReset,
		&i.CacheHitRatio,
		&i.StatsAgeDays,
	)
	return i, err
}

const databaseFreezeAge = `-- name: DatabaseFreezeAge :many
SELECT
  datname::text AS database_name
  , datfrozenxid::text AS frozen_xid
  , age(datfrozenxid) AS freeze_age
  , (
    SELECT s.setting::bigint FROM pg_settings AS s
    WHERE s.name = 'autovacuum_freeze_max_age'
  ) AS freeze_max_age
FROM pg_database
WHERE datallowconn = true
ORDER BY age(datfrozenxid) DESC
`

type DatabaseFreezeAgeRow struct {
	DatabaseName pgtype.Text
	FrozenXid    pgtype.Text
	FreezeAge    pgtype.Int4
	FreezeMaxAge pgtype.Int8
}

// Gets transaction ID age for all databases.
func (q *Queries) DatabaseFreezeAge(ctx context.Context) ([]DatabaseFreezeAgeRow, error) {
	rows, err := q.db.Query(ctx, databaseFreezeAge)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []DatabaseFreezeAgeRow
	for rows.Next() {
		var i DatabaseFreezeAgeRow
		if err := rows.Scan(
			&i.DatabaseName,
			&i.FrozenXid,
			&i.FreezeAge,
			&i.FreezeMaxAge,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const duplicateIndexes = `-- name: DuplicateIndexes :many
WITH index_columns AS (
  SELECT
    idx.indexrelid
    , idx.indrelid
    , i.relname AS index_name
    , t.relname AS table_name
    , n.nspname AS schema_name
    , idx.indkey::int [] AS column_positions
    , idx.indnkeyatts AS num_key_columns
    -- Extract column list as array for prefix comparison
    , pg_get_indexdef(idx.indexrelid) AS index_def
    , pg_relation_size(i.oid) AS index_size_bytes
    -- Detect expression/partial indexes (cannot reliably compare)
    , (idx.indexprs IS NOT NULL) AS is_expression_index
    , (idx.indpred IS NOT NULL) AS is_partial_index
  FROM pg_index AS idx
  INNER JOIN pg_class AS i ON idx.indexrelid = i.oid
  INNER JOIN pg_class AS t ON idx.indrelid = t.oid
  INNER JOIN pg_namespace AS n ON t.relnamespace = n.oid
  WHERE
    i.relkind = 'i'
    AND idx.indisvalid
    AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast', 'cron', 'pgpartman', 'debezium')
)

, exact_duplicates AS (
  -- Find indexes with identical definitions (after removing index name)
  SELECT
    a.schema_name
    , a.table_name
    , a.index_name AS index_name_a
    , b.index_name AS index_name_b
    , a.index_size_bytes AS size_a
    , b.index_size_bytes AS size_b
    , a.index_def AS definition_a
    , 'exact' AS duplicate_type
  FROM index_columns AS a
  INNER JOIN index_columns AS b ON
    a.indrelid = b.indrelid
    AND a.indexrelid < b.indexrelid
    AND regexp_replace(a.index_def, 'INDEX \S+ ON', 'INDEX ON', 'g')
    = regexp_replace(b.index_def, 'INDEX \S+ ON', 'INDEX ON', 'g')
)

, prefix_duplicates AS (
  -- Find indexes where one is a left-prefix of another
  -- e.g., (a) is prefix of (a, b)
  SELECT
    a.schema_name
    , a.table_name
    , a.index_name AS index_name_a
    , b.index_name AS index_name_b
    , a.index_size_bytes AS size_a
    , b.index_size_bytes AS size_b
    , a.index_def AS definition_a
    , 'prefix' AS duplicate_type
  FROM index_columns AS a
  INNER JOIN index_columns AS b ON
    a.indrelid = b.indrelid
    AND a.indexrelid <> b.indexrelid
    AND a.num_key_columns < b.num_key_columns
    AND a.column_positions = b.column_positions[0:a.num_key_columns]
    AND NOT a.is_expression_index
    AND NOT b.is_expression_index
    AND NOT a.is_partial_index
    AND NOT b.is_partial_index
)

SELECT
  (schema_name || '.' || table_name)::text AS table_name
  , index_name_a::text
  , index_name_b::text
  , size_a
  , size_b
  , definition_a::text
  , duplicate_type::text
FROM (
  SELECT
    schema_name
    , table_name
    , index_name_a
    , index_name_b
    , size_a
    , size_b
    , definition_a
    , duplicate_type
  FROM exact_duplicates
  UNION ALL
  SELECT
    schema_name
    , table_name
    , index_name_a
    , index_name_b
    , size_a
    , size_b
    , definition_a
    , duplicate_type
  FROM prefix_duplicates
) AS all_duplicates
ORDER BY
  size_a + size_b DESC
`

type DuplicateIndexesRow struct {
	TableName     pgtype.Text
	IndexNameA    pgtype.Text
	IndexNameB    pgtype.Text
	SizeA         pgtype.Int8
	SizeB         pgtype.Int8
	DefinitionA   pgtype.Text
	DuplicateType pgtype.Text
}

// Identifies exact and prefix duplicate indexes on the same table.
// Uses index column positions (indkey) for prefix detection.
// Excludes: system schemas, invalid indexes, expression/partial indexes for prefix check.
func (q *Queries) DuplicateIndexes(ctx context.Context) ([]DuplicateIndexesRow, error) {
	rows, err := q.db.Query(ctx, duplicateIndexes)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []DuplicateIndexesRow
	for rows.Next() {
		var i DuplicateIndexesRow
		if err := rows.Scan(
			&i.TableName,
			&i.IndexNameA,
			&i.IndexNameB,
			&i.SizeA,
			&i.SizeB,
			&i.DefinitionA,
			&i.DuplicateType,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const hasPgStatStatements = `-- name: HasPgStatStatements :one
SELECT EXISTS(
  SELECT 1 FROM pg_extension
  WHERE extname = 'pg_stat_statements'
)
`

// Checks if pg_stat_statements extension is installed.
func (q *Queries) HasPgStatStatements(ctx context.Context) (bool, error) {
	row := q.db.QueryRow(ctx, hasPgStatStatements)
	var exists bool
	err := row.Scan(&exists)
	return exists, err
}

const highSeqScanTables = `-- name: HighSeqScanTables :many
WITH table_indexes AS (
  SELECT
    idx.indrelid AS table_oid
    , count(*) AS index_count
  FROM pg_index AS idx
  WHERE idx.indisvalid
  GROUP BY idx.indrelid
)

SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , coalesce(s.seq_scan, 0) AS seq_scan
  , coalesce(s.idx_scan, 0) AS idx_scan
  , CASE
    WHEN coalesce(s.idx_scan, 0) = 0 THEN NULL
    ELSE round(s.seq_scan::numeric / s.idx_scan, 2)
  END AS seq_to_idx_ratio
  , coalesce(s.n_live_tup, 0) AS estimated_rows
  , pg_relation_size(c.oid) AS table_size_bytes
  , coalesce(ti.index_count, 0) AS index_count
FROM pg_class AS c
INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
LEFT JOIN table_indexes AS ti ON c.oid = ti.table_oid
WHERE
  c.relkind IN ('r', 'p')
  AND n.nspname = 'public'
  AND coalesce(s.n_live_tup, 0) > 10000
  AND coalesce(s.seq_scan, 0) > 100
ORDER BY
  coalesce(s.seq_scan, 0) DESC
`

type HighSeqScanTablesRow struct {
	TableName      pgtype.Text
	SeqScan        pgtype.Int8
	IdxScan        pgtype.Int8
	SeqToIdxRatio  pgtype.Numeric
	EstimatedRows  pgtype.Int8
	TableSizeBytes pgtype.Int8
	IndexCount     pgtype.Int8
}

// Identifies tables with excessive sequential scans relative to index scans.
// Excludes: small tables, system schemas, tables with no indexes.
func (q *Queries) HighSeqScanTables(ctx context.Context) ([]HighSeqScanTablesRow, error) {
	rows, err := q.db.Query(ctx, highSeqScanTables)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []HighSeqScanTablesRow
	for rows.Next() {
		var i HighSeqScanTablesRow
		if err := rows.Scan(
			&i.TableName,
			&i.SeqScan,
			&i.IdxScan,
			&i.SeqToIdxRatio,
			&i.EstimatedRows,
			&i.TableSizeBytes,
			&i.IndexCount,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const idleInTransaction = `-- name: IdleInTransaction :many
SELECT
  pg_stat_activity.pid
  , pg_stat_activity.usename::text AS username
  , pg_stat_activity.datname::text AS database_name
  , pg_stat_activity.application_name::text AS application_name
  , pg_stat_activity.state::text AS state
  , extract(EPOCH FROM (now() - pg_stat_activity.xact_start))::bigint AS transaction_duration_seconds
  , left(pg_stat_activity.query, 200)::text AS query_preview
  , coalesce((
    SELECT pg_settings.setting::bigint
    FROM pg_settings
    WHERE pg_settings.name = 'idle_in_transaction_session_timeout'
  ), 0) AS timeout_ms
FROM pg_stat_activity
WHERE
  pg_stat_activity.state IN ('idle in transaction', 'idle in transaction (aborted)')
  AND pg_stat_activity.pid != pg_backend_pid()
ORDER BY pg_stat_activity.xact_start ASC
`

type IdleInTransactionRow struct {
	Pid                        pgtype.Int4
	Username                   pgtype.Text
	DatabaseName               pgtype.Text
	ApplicationName            pgtype.Text
	State                      pgtype.Text
	TransactionDurationSeconds pgtype.Int8
	QueryPreview               pgtype.Text
	TimeoutMs                  pgtype.Int8
}

// Identifies connections stuck in 'idle in transaction' state.
// Includes the timeout setting (in ms) for threshold calculation in Go.
func (q *Queries) IdleInTransaction(ctx context.Context) ([]IdleInTransactionRow, error) {
	rows, err := q.db.Query(ctx, idleInTransaction)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []IdleInTransactionRow
	for rows.Next() {
		var i IdleInTransactionRow
		if err := rows.Scan(
			&i.Pid,
			&i.Username,
			&i.DatabaseName,
			&i.ApplicationName,
			&i.State,
			&i.TransactionDurationSeconds,
			&i.QueryPreview,
			&i.TimeoutMs,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const indexBloat = `-- name: IndexBloat :many
WITH index_info AS (
  SELECT
    n.nspname::text AS schemaname
    , t.relname::text AS tablename
    , i.relname::text AS indexname
    , t.oid AS table_oid
    , i.relpages AS actual_pages
    , i.reltuples
    , ix.indkey
    , CURRENT_SETTING('block_size')::int AS bs
    , COALESCE(
      SUBSTRING(ARRAY_TO_STRING(i.reloptions, ' ') FROM 'fillfactor=([0-9]+)')::int
      , 90
    ) AS fill_factor
  FROM pg_index AS ix
  INNER JOIN pg_class AS i ON ix.indexrelid = i.oid
  INNER JOIN pg_class AS t ON ix.indrelid = t.oid
  INNER JOIN pg_namespace AS n ON i.relnamespace = n.oid
  INNER JOIN pg_am AS am ON i.relam = am.oid
  WHERE
    am.amname = 'btree'
    AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
    AND i.relpages > 100  -- Skip tiny indexes (<800KB)
    AND ix.indisvalid
    AND i.reltuples > 0
)

, index_columns AS (
  SELECT
    ii.schemaname
    , ii.tablename
    , ii.indexname
    , ii.actual_pages
    , ii.reltuples
    , ii.fill_factor
    , ii.bs
    -- Sum avg_width from pg_stats for indexed columns
    -- Fallback to 24 bytes if no stats (reasonable for UUID/timestamp)
    , COALESCE(
      (
        SELECT SUM(COALESCE(s.avg_width, 8))
        FROM UNNEST(ii.indkey) WITH ORDINALITY AS u (attnum, ord)
        INNER JOIN pg_attribute AS a ON a.attrelid = ii.table_oid AND u.attnum = a.attnum
        LEFT JOIN pg_stats AS s
          ON
            s.schemaname = ii.schemaname
            AND s.tablename = ii.tablename
            AND a.attname = s.attname
        WHERE u.attnum > 0
      )
      , 24
    ) AS data_width
  FROM index_info AS ii
)

, bloat_calc AS (
  SELECT
    schemaname
    , tablename
    , indexname
    , actual_pages
    , reltuples
    , bs
    , data_width
    -- Index tuple size: ItemPointer(6) + info(2) + data ≈ 8 + data_width
    -- Simplified: skip per-column MAXALIGN, add ~20% padding estimate
    , CEIL((8 + data_width) * 1.2) AS tuple_size
    -- Usable space: block_size - PageHeader(24) - BTPageOpaque(16), apply fill_factor
    , FLOOR((bs - 40) * fill_factor / 100.0) AS usable_space
  FROM index_columns
)

, bloat_estimate AS (
  SELECT
    schemaname
    , tablename
    , indexname
    , actual_pages
    , bs
    -- Expected pages = ceil(tuples / (usable_space / (line_pointer(4) + tuple_size)))
    , GREATEST(1, CEIL(reltuples / FLOOR(usable_space / (4 + tuple_size))))::bigint AS est_pages
    , (actual_pages::bigint * bs) AS actual_bytes
  FROM bloat_calc
  WHERE tuple_size > 0 AND usable_space > (4 + tuple_size)
)

SELECT
  schemaname
  , tablename
  , indexname
  , actual_pages
  , est_pages
  , actual_bytes
  , ((actual_pages - est_pages)::bigint * bs) AS bloat_bytes
  , CASE
    WHEN actual_pages > 0 AND actual_pages > est_pages
      THEN ROUND(100.0 * (actual_pages - est_pages) / actual_pages, 1)
    ELSE 0
  END AS bloat_percent
FROM bloat_estimate
WHERE actual_pages > est_pages
ORDER BY bloat_percent DESC, bloat_bytes DESC
`

type IndexBloatRow struct {
	Schemaname   pgtype.Text
	Tablename    pgtype.Text
	Indexname    pgtype.Text
	ActualPages  int32
	EstPages     pgtype.Int8
	ActualBytes  pgtype.Int8
	BloatBytes   pgtype.Int8
	BloatPercent pgtype.Numeric
}

// Balanced B-tree index bloat estimation using pg_stats column widths
// Accuracy: ±15% (good enough for health checks, not precision measurement)
func (q *Queries) IndexBloat(ctx context.Context) ([]IndexBloatRow, error) {
	rows, err := q.db.Query(ctx, indexBloat)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []IndexBloatRow
	for rows.Next() {
		var i IndexBloatRow
		if err := rows.Scan(
			&i.Schemaname,
			&i.Tablename,
			&i.Indexname,
			&i.ActualPages,
			&i.EstPages,
			&i.ActualBytes,
			&i.BloatBytes,
			&i.BloatPercent,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const indexUsageStats = `-- name: IndexUsageStats :many
SELECT
  (n.nspname || '.' || tbl.relname)::text AS table_name
  , psai.indexrelname::text AS index_name
  , c.reltuples::bigint AS num_rows
  , x.indisprimary AS is_primary
  , x.indisunique AS is_unique
  , pg_relation_size(psai.indexrelid) AS index_size_bytes
  , coalesce(psai.idx_scan, 0) AS idx_scan
  , coalesce(psai.idx_tup_read, 0) AS idx_tup_read
  , coalesce(psai.idx_tup_fetch, 0) AS idx_tup_fetch
  , coalesce(ut.n_tup_ins, 0) + coalesce(ut.n_tup_upd, 0) + coalesce(ut.n_tup_del, 0) AS table_writes
  , coalesce(psaio.idx_blks_hit, 0) AS idx_blks_hit
  , coalesce(psaio.idx_blks_read, 0) AS idx_blks_read
  , CASE
    WHEN coalesce(psaio.idx_blks_hit, 0) + coalesce(psaio.idx_blks_read, 0) = 0 THEN NULL
    ELSE round(
      100.0 * psaio.idx_blks_hit / (psaio.idx_blks_hit + psaio.idx_blks_read)
      , 2
    )
  END AS cache_hit_ratio
  , pg_get_indexdef(psai.indexrelid) AS indexdef
FROM pg_stat_user_indexes AS psai
INNER JOIN pg_index AS x ON psai.indexrelid = x.indexrelid
INNER JOIN pg_class AS tbl ON x.indrelid = tbl.oid
INNER JOIN pg_namespace AS n ON tbl.relnamespace = n.oid
LEFT JOIN pg_class AS c ON psai.relid = c.oid
LEFT JOIN pg_stat_user_tables AS ut ON tbl.oid = ut.relid
LEFT JOIN pg_statio_user_indexes AS psaio ON psai.indexrelid = psaio.indexrelid
WHERE
  n.nspname = 'public'
ORDER BY
  pg_relation_size(psai.indexrelid) DESC
`

type IndexUsageStatsRow struct {
	TableName      pgtype.Text
	IndexName      pgtype.Text
	NumRows        pgtype.Int8
	IsPrimary      bool
	IsUnique       bool
	IndexSizeBytes pgtype.Int8
	IdxScan        pgtype.Int8
	IdxTupRead     pgtype.Int8
	IdxTupFetch    pgtype.Int8
	TableWrites    pgtype.Int8
	IdxBlksHit     pgtype.Int8
	IdxBlksRead    pgtype.Int8
	CacheHitRatio  pgtype.Numeric
	Indexdef       pgtype.Text
}

// Identifies indexes with usage statistics for health analysis.
// Excludes: system schemas.
// Returns data for subchecks: unused-indexes, low-usage-indexes, index-cache-ratio.
func (q *Queries) IndexUsageStats(ctx context.Context) ([]IndexUsageStatsRow, error) {
	rows, err := q.db.Query(ctx, indexUsageStats)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []IndexUsageStatsRow
	for rows.Next() {
		var i IndexUsageStatsRow
		if err := rows.Scan(
			&i.TableName,
			&i.IndexName,
			&i.NumRows,
			&i.IsPrimary,
			&i.IsUnique,
			&i.IndexSizeBytes,
			&i.IdxScan,
			&i.IdxTupRead,
			&i.IdxTupFetch,
			&i.TableWrites,
			&i.IdxBlksHit,
			&i.IdxBlksRead,
			&i.CacheHitRatio,
			&i.Indexdef,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const invalidPrimaryKeyTypes = `-- name: InvalidPrimaryKeyTypes :many
WITH pk_tables AS (
  SELECT
    n.nspname::text AS schema_name
    , c.relname::text AS table_name
    , a.attname::text AS column_name
    , a.attnum AS column_num
    , t.typname::text AS column_type
    , c.oid AS table_oid
    , COALESCE(s.n_live_tup, 0)::bigint AS estimated_rows
    , CASE t.typname
      WHEN 'int2' THEN 32767::bigint
      WHEN 'int4' THEN 2147483647::bigint
    END AS type_max_value
  FROM pg_catalog.pg_constraint AS con
  INNER JOIN pg_catalog.pg_class AS c ON con.conrelid = c.oid
  INNER JOIN pg_catalog.pg_namespace AS n ON c.relnamespace = n.oid
  INNER JOIN pg_catalog.pg_attribute AS a
    ON
      con.conrelid = a.attrelid
      AND a.attnum = ANY(con.conkey)
  INNER JOIN pg_catalog.pg_type AS t ON a.atttypid = t.oid
  LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
  WHERE
    con.contype = 'p'
    AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pgpartman', 'pgjobmon', 'cron')
    AND t.typname IN ('int2', 'int4')
    AND NOT EXISTS (
      SELECT 1 FROM pg_inherits AS inh
      WHERE inh.inhrelid = c.oid
    )
)

, sequence_values AS (
  SELECT
    d.refobjid AS table_oid
    , d.refobjsubid AS column_num
    , seq.last_value::bigint AS sequence_current
  FROM pg_depend AS d
  INNER JOIN pg_class AS seq_class ON d.objid = seq_class.oid
  INNER JOIN pg_sequences AS seq ON seq_class.relname = seq.sequencename
  WHERE
    d.deptype = 'a'
    AND seq_class.relkind = 'S'
)

, pk_with_usage AS (
  SELECT
    (p.schema_name || '.' || p.table_name)::text AS table_name
    , p.column_name
    , p.column_type
    , p.estimated_rows
    , sv.sequence_current
    , p.type_max_value
    , CASE
      WHEN sv.sequence_current IS NOT NULL AND p.type_max_value > 0
        THEN sv.sequence_current::numeric / p.type_max_value::numeric
      WHEN p.estimated_rows > 0 AND p.type_max_value > 0
        THEN p.estimated_rows::numeric / p.type_max_value::numeric
      ELSE
        0::numeric
    END AS usage_pct
  FROM pk_tables AS p
  LEFT JOIN sequence_values AS sv
    ON
      p.table_oid = sv.table_oid
      AND p.column_num = sv.column_num
)

SELECT
  table_name
  , column_name
  , column_type
  , estimated_rows
  , sequence_current
  , type_max_value
  , usage_pct
FROM pk_with_usage
ORDER BY
  usage_pct DESC NULLS LAST
  , estimated_rows DESC NULLS LAST
`

type InvalidPrimaryKeyTypesRow struct {
	TableName       pgtype.Text
	ColumnName      pgtype.Text
	ColumnType      pgtype.Text
	EstimatedRows   pgtype.Int8
	SequenceCurrent pgtype.Int8
	TypeMaxValue    pgtype.Int8
	UsagePct        pgtype.Numeric
}

// Identifies tables with integer primary keys (int2/int4) that should use bigint.
func (q *Queries) InvalidPrimaryKeyTypes(ctx context.Context) ([]InvalidPrimaryKeyTypesRow, error) {
	rows, err := q.db.Query(ctx, invalidPrimaryKeyTypes)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []InvalidPrimaryKeyTypesRow
	for rows.Next() {
		var i InvalidPrimaryKeyTypesRow
		if err := rows.Scan(
			&i.TableName,
			&i.ColumnName,
			&i.ColumnType,
			&i.EstimatedRows,
			&i.SequenceCurrent,
			&i.TypeMaxValue,
			&i.UsagePct,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const largeTables = `-- name: LargeTables :many
WITH inheritance_info AS (
  SELECT DISTINCT ON (i.inhrelid)
    i.inhrelid AS child_oid
    , (pn.nspname || '.' || pc.relname)::text AS parent_table
  FROM pg_inherits AS i
  INNER JOIN pg_class AS pc ON i.inhparent = pc.oid
  INNER JOIN pg_namespace AS pn ON pc.relnamespace = pn.oid
  ORDER BY i.inhrelid, i.inhparent
)

SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , ii.parent_table
  , pg_catalog.pg_table_size(c.oid) AS table_size_bytes
  , COALESCE(s.n_live_tup, 0) AS estimated_rows
  , (c.relkind = 'p') AS is_partitioned
  , (ii.parent_table IS NOT NULL) AS is_partition
  , (c.relname ~ '(outbox|inbox|_jobs?$|^oban_|logs|events?$)') AS is_transient
  , COALESCE(s.n_tup_ins, 0) AS n_tup_ins
  , COALESCE(s.n_tup_upd, 0) AS n_tup_upd
  , COALESCE(s.n_tup_del, 0) AS n_tup_del
FROM pg_catalog.pg_class AS c
INNER JOIN pg_catalog.pg_namespace AS n ON c.relnamespace = n.oid
LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
LEFT JOIN inheritance_info AS ii ON c.oid = ii.child_oid
WHERE
  c.relkind IN ('r', 'p')
  AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast', 'pgpartman', 'debezium', 'cron')
  AND COALESCE(s.n_live_tup, 0) >= 10000000
`

type LargeTablesRow struct {
	TableName      pgtype.Text
	ParentTable    pgtype.Text
	TableSizeBytes pgtype.Int8
	EstimatedRows  pgtype.Int8
	IsPartitioned  pgtype.Bool
	IsPartition    pgtype.Bool
	IsTransient    pgtype.Bool
	NTupIns        pgtype.Int8
	NTupUpd        pgtype.Int8
	NTupDel        pgtype.Int8
}

// Identifies all large tables (>= 10M rows) with partitioning and transient status.
// Returns both regular and partitioned tables for unified analysis.
// Includes activity metrics (inserts/updates/deletes) for activity-aware thresholds.
func (q *Queries) LargeTables(ctx context.Context) ([]LargeTablesRow, error) {
	rows, err := q.db.Query(ctx, largeTables)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []LargeTablesRow
	for rows.Next() {
		var i LargeTablesRow
		if err := rows.Scan(
			&i.TableName,
			&i.ParentTable,
			&i.TableSizeBytes,
			&i.EstimatedRows,
			&i.IsPartitioned,
			&i.IsPartition,
			&i.IsTransient,
			&i.NTupIns,
			&i.NTupUpd,
			&i.NTupDel,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const longIdleConnections = `-- name: LongIdleConnections :many
SELECT
  pid
  , usename::text AS username
  , datname::text AS database_name
  , application_name::text AS application_name
  , client_addr::text AS client_address
  , state::text AS state
  , extract(EPOCH FROM (now() - state_change))::bigint AS idle_duration_seconds
  , extract(EPOCH FROM (now() - backend_start))::bigint AS connection_age_seconds
FROM pg_stat_activity
WHERE
  state = 'idle'
  AND pid != pg_backend_pid()
  AND (now() - state_change) > interval '30 minutes'
ORDER BY state_change ASC
`

type LongIdleConnectionsRow struct {
	Pid                  pgtype.Int4
	Username             pgtype.Text
	DatabaseName         pgtype.Text
	ApplicationName      pgtype.Text
	ClientAddress        pgtype.Text
	State                pgtype.Text
	IdleDurationSeconds  pgtype.Int8
	ConnectionAgeSeconds pgtype.Int8
}

// Identifies connections that have been idle for too long (potential pool leak).
func (q *Queries) LongIdleConnections(ctx context.Context) ([]LongIdleConnectionsRow, error) {
	rows, err := q.db.Query(ctx, longIdleConnections)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []LongIdleConnectionsRow
	for rows.Next() {
		var i LongIdleConnectionsRow
		if err := rows.Scan(
			&i.Pid,
			&i.Username,
			&i.DatabaseName,
			&i.ApplicationName,
			&i.ClientAddress,
			&i.State,
			&i.IdleDurationSeconds,
			&i.ConnectionAgeSeconds,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const missingProviderIdTables = `-- name: MissingProviderIdTables :many
WITH user_tables AS (
  SELECT
    (n.nspname || '.' || c.relname)::text AS table_name
    , c.oid AS table_oid
    , pg_catalog.pg_table_size(c.oid) AS table_size_bytes
    , CASE
      WHEN c.relkind = 'p'
        THEN (
          -- For partitioned tables, sum stats from all child partitions
          SELECT COALESCE(SUM(child_stats.n_live_tup), 0)::bigint
          FROM pg_catalog.pg_inherits AS i
          INNER JOIN pg_stat_user_tables AS child_stats ON i.inhrelid = child_stats.relid
          WHERE i.inhparent = c.oid
        )
      ELSE COALESCE(s.n_live_tup, 0)
    END AS estimated_rows
  FROM pg_catalog.pg_class AS c
  INNER JOIN pg_catalog.pg_namespace AS n ON c.relnamespace = n.oid
  LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
  WHERE
    c.relkind IN ('r', 'p')
    AND n.nspname = 'public'
)

, tables_with_provider_id AS (
  SELECT DISTINCT a.attrelid AS table_oid
  FROM pg_catalog.pg_attribute AS a
  WHERE
    a.attname = 'provider_id'
    AND a.attnum > 0
    AND NOT a.attisdropped
)

SELECT
  CURRENT_DATABASE()::text AS database_name
  , ut.table_name
  , ut.table_size_bytes
  , ut.estimated_rows
FROM user_tables AS ut
LEFT JOIN tables_with_provider_id AS t ON ut.table_oid = t.table_oid
WHERE t.table_oid IS NULL
ORDER BY ut.table_size_bytes DESC
`

type MissingProviderIdTablesRow struct {
	DatabaseName   pgtype.Text
	TableName      pgtype.Text
	TableSizeBytes pgtype.Int8
	EstimatedRows  pgtype.Int8
}

// Identifies tables without provider_id column for multi-tenancy support.
func (q *Queries) MissingProviderIdTables(ctx context.Context) ([]MissingProviderIdTablesRow, error) {
	rows, err := q.db.Query(ctx, missingProviderIdTables)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []MissingProviderIdTablesRow
	for rows.Next() {
		var i MissingProviderIdTablesRow
		if err := rows.Scan(
			&i.DatabaseName,
			&i.TableName,
			&i.TableSizeBytes,
			&i.EstimatedRows,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const pGVersion = `-- name: PGVersion :one
SELECT
  current_setting('server_version_num')::integer / 10000 AS major
  , current_setting('server_version_num')::integer % 100 AS minor
`

type PGVersionRow struct {
	Major int32
	Minor int32
}

func (q *Queries) PGVersion(ctx context.Context) (PGVersionRow, error) {
	row := q.db.QueryRow(ctx, pGVersion)
	var i PGVersionRow
	err := row.Scan(&i.Major, &i.Minor)
	return i, err
}

const partitionedTablesWithKeys = `-- name: PartitionedTablesWithKeys :many
WITH partition_stats AS (
  -- Single aggregation of all partition metrics from child tables
  SELECT
    i.inhparent
    , COUNT(*)::bigint AS partition_count
    , COALESCE(SUM(pg_catalog.pg_total_relation_size(i.inhrelid)), 0)::bigint AS total_size_bytes
    , COALESCE(SUM(s.n_live_tup), 0)::bigint AS estimated_rows
    , COALESCE(SUM(s.seq_scan), 0)::bigint AS total_seq_scans
    , COALESCE(SUM(s.idx_scan), 0)::bigint AS total_idx_scans
  FROM pg_inherits AS i
  LEFT JOIN pg_stat_user_tables AS s ON i.inhrelid = s.relid
  GROUP BY i.inhparent
)

SELECT
  n.nspname::text AS schema_name
  , c.relname::text AS table_name
  , pt.partstrat::text AS partition_strategy
  -- Get partition key column names as comma-separated string
  , (
    SELECT STRING_AGG(a.attname, ',' ORDER BY k.n)
    FROM UNNEST(pt.partattrs) WITH ORDINALITY AS k (attnum, n)
    INNER JOIN pg_attribute AS a ON a.attrelid = c.oid AND k.attnum = a.attnum
    WHERE k.attnum > 0
  )::text AS partition_key_columns
  -- Check if partition key includes expressions (attnum = 0 means expression)
  , (SELECT BOOL_OR(k.attnum = 0) FROM UNNEST(pt.partattrs) AS k (attnum)) AS has_expression_key
  -- All partition metrics from pre-aggregated CTE
  , COALESCE(ps.partition_count, 0) AS partition_count
  , COALESCE(ps.total_size_bytes, 0) AS total_size_bytes
  , COALESCE(ps.estimated_rows, 0) AS estimated_rows
  , COALESCE(ps.total_seq_scans, 0) AS total_seq_scans
  , COALESCE(ps.total_idx_scans, 0) AS total_idx_scans
FROM pg_catalog.pg_class AS c
INNER JOIN pg_catalog.pg_namespace AS n ON c.relnamespace = n.oid
INNER JOIN pg_partitioned_table AS pt ON c.oid = pt.partrelid
LEFT JOIN partition_stats AS ps ON c.oid = ps.inhparent
WHERE
  c.relkind = 'p'
  AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast', 'pgpartman', 'debezium', 'cron')
ORDER BY ps.total_size_bytes DESC NULLS LAST
`

type PartitionedTablesWithKeysRow struct {
	SchemaName          pgtype.Text
	TableName           pgtype.Text
	PartitionStrategy   pgtype.Text
	PartitionKeyColumns pgtype.Text
	HasExpressionKey    pgtype.Bool
	PartitionCount      pgtype.Int8
	TotalSizeBytes      pgtype.Int8
	EstimatedRows       pgtype.Int8
	TotalSeqScans       pgtype.Int8
	TotalIdxScans       pgtype.Int8
}

// Gets partitioned tables and their partition key column(s).
// Pre-aggregates all partition statistics in a single CTE for better performance
// compared to multiple correlated subqueries.
func (q *Queries) PartitionedTablesWithKeys(ctx context.Context) ([]PartitionedTablesWithKeysRow, error) {
	rows, err := q.db.Query(ctx, partitionedTablesWithKeys)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []PartitionedTablesWithKeysRow
	for rows.Next() {
		var i PartitionedTablesWithKeysRow
		if err := rows.Scan(
			&i.SchemaName,
			&i.TableName,
			&i.PartitionStrategy,
			&i.PartitionKeyColumns,
			&i.HasExpressionKey,
			&i.PartitionCount,
			&i.TotalSizeBytes,
			&i.EstimatedRows,
			&i.TotalSeqScans,
			&i.TotalIdxScans,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const queryStatsFromStatStatements = `-- name: QueryStatsFromStatStatements :many
SELECT
  queryid::bigint AS query_id
  , LEFT(REGEXP_REPLACE(query, '\s+', ' ', 'g'), 80)::text AS query
  , calls::bigint AS calls
  , total_exec_time::double precision AS total_exec_time
  , mean_exec_time::double precision AS mean_exec_time
  , rows::bigint AS rows_returned
FROM pg_stat_statements
WHERE
  calls > 10
  AND query NOT LIKE 'COPY%'
  AND query NOT LIKE 'SET %'
  AND query !~ '^(BEGIN|COMMIT|ROLLBACK|SAVEPOINT|PREPARE|DEALLOCATE)'
  AND query !~ '^(VACUUM|ANALYZE|REINDEX|CLUSTER)'
  AND query !~ '^(CREATE|DROP|ALTER|TRUNCATE)'
  AND (query ILIKE '%SELECT%' OR query ILIKE '%UPDATE%' OR query ILIKE '%DELETE%')
ORDER BY total_exec_time DESC
LIMIT 500
`

type QueryStatsFromStatStatementsRow struct {
	QueryID       pgtype.Int8
	Query         pgtype.Text
	Calls         pgtype.Int8
	TotalExecTime pgtype.Float8
	MeanExecTime  pgtype.Float8
	RowsReturned  pgtype.Int8
}

// Gets query statistics from pg_stat_statements for partition key analysis.
// Returns queries with significant usage to check against partitioned tables.
func (q *Queries) QueryStatsFromStatStatements(ctx context.Context) ([]QueryStatsFromStatStatementsRow, error) {
	rows, err := q.db.Query(ctx, queryStatsFromStatStatements)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []QueryStatsFromStatStatementsRow
	for rows.Next() {
		var i QueryStatsFromStatStatementsRow
		if err := rows.Scan(
			&i.QueryID,
			&i.Query,
			&i.Calls,
			&i.TotalExecTime,
			&i.MeanExecTime,
			&i.RowsReturned,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const replicationLag = `-- name: ReplicationLag :many
SELECT
  -- Consumer/replica identity
  sr.application_name::text AS application_name
  , sr.state::text AS state

  -- Replication type detection (authoritative: slot_type, fallback: sync_state presence)
  , CASE
    WHEN rs.slot_type IS NOT NULL THEN rs.slot_type
    WHEN sr.sync_state IS NOT NULL THEN 'physical'
    ELSE 'unknown'
  END::text AS replication_type

  -- Lag metrics (bytes) - cast to bigint to get native int64
  , COALESCE(PG_WAL_LSN_DIFF(PG_CURRENT_WAL_LSN(), sr.replay_lsn), 0)::bigint AS replay_lag_bytes

  -- Lag metrics (seconds) - cast to float8 (double precision) to get native float64
  , COALESCE(EXTRACT(EPOCH FROM sr.replay_lag), 0)::float8 AS replay_lag_seconds

  -- Associated replication slot (NULL if no slot)
  , rs.slot_name::text AS slot_name
  , rs.wal_status::text AS wal_status

FROM pg_stat_replication AS sr
LEFT JOIN pg_replication_slots AS rs ON sr.pid = rs.active_pid
ORDER BY
  EXTRACT(EPOCH FROM sr.replay_lag) DESC NULLS LAST
  , sr.application_name
`

type ReplicationLagRow struct {
	ApplicationName  pgtype.Text
	State            pgtype.Text
	ReplicationType  pgtype.Text
	ReplayLagBytes   pgtype.Int8
	ReplayLagSeconds pgtype.Float8
	SlotName         pgtype.Text
	WalStatus        pgtype.Text
}

// Monitors replication lag for both physical and logical replication streams.
// Joins with pg_replication_slots to get authoritative replication type and slot information.
func (q *Queries) ReplicationLag(ctx context.Context) ([]ReplicationLagRow, error) {
	rows, err := q.db.Query(ctx, replicationLag)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ReplicationLagRow
	for rows.Next() {
		var i ReplicationLagRow
		if err := rows.Scan(
			&i.ApplicationName,
			&i.State,
			&i.ReplicationType,
			&i.ReplayLagBytes,
			&i.ReplayLagSeconds,
			&i.SlotName,
			&i.WalStatus,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const replicationSlots = `-- name: ReplicationSlots :many
SELECT
  slot_name
  , slot_type
  , plugin
  , database
  , active
  , active_pid
  , wal_status
  , safe_wal_size
  , temporary
  , conflicting
  , invalidation_reason
  , PG_WAL_LSN_DIFF(PG_CURRENT_WAL_LSN(), restart_lsn)::BIGINT AS restart_lsn_lag_bytes
  , PG_WAL_LSN_DIFF(PG_CURRENT_WAL_LSN(), confirmed_flush_lsn)::BIGINT AS confirmed_flush_lsn_lag_bytes
  , CASE
    WHEN active THEN NULL
    ELSE EXTRACT(EPOCH FROM (NOW() - inactive_since))::BIGINT
  END AS inactive_seconds

FROM pg_replication_slots
ORDER BY
  CASE
    WHEN NOT active THEN 1
    WHEN wal_status = 'lost' THEN 2
    WHEN wal_status = 'unreserved' THEN 3
    ELSE 4
  END
  , restart_lsn_lag_bytes DESC NULLS LAST
`

type ReplicationSlotsRow struct {
	SlotName                  pgtype.Text
	SlotType                  pgtype.Text
	Plugin                    pgtype.Text
	Database                  pgtype.Text
	Active                    pgtype.Bool
	ActivePid                 pgtype.Int4
	WalStatus                 pgtype.Text
	SafeWalSize               pgtype.Int8
	Temporary                 pgtype.Bool
	Conflicting               pgtype.Bool
	InvalidationReason        pgtype.Text
	RestartLsnLagBytes        pgtype.Int8
	ConfirmedFlushLsnLagBytes pgtype.Int8
	InactiveSeconds           pgtype.Int8
}

// For PostgreSQL 17+: includes inactive_since, conflicting, invalidation_reason
func (q *Queries) ReplicationSlots(ctx context.Context) ([]ReplicationSlotsRow, error) {
	rows, err := q.db.Query(ctx, replicationSlots)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ReplicationSlotsRow
	for rows.Next() {
		var i ReplicationSlotsRow
		if err := rows.Scan(
			&i.SlotName,
			&i.SlotType,
			&i.Plugin,
			&i.Database,
			&i.Active,
			&i.ActivePid,
			&i.WalStatus,
			&i.SafeWalSize,
			&i.Temporary,
			&i.Conflicting,
			&i.InvalidationReason,
			&i.RestartLsnLagBytes,
			&i.ConfirmedFlushLsnLagBytes,
			&i.InactiveSeconds,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const replicationSlotsPG15 = `-- name: ReplicationSlotsPG15 :many
SELECT
  slot_name
  , slot_type
  , plugin
  , database
  , active
  , active_pid
  , wal_status
  , safe_wal_size
  , temporary
  , NULL::BOOLEAN AS conflicting
  , NULL::TEXT AS invalidation_reason
  , PG_WAL_LSN_DIFF(PG_CURRENT_WAL_LSN(), restart_lsn)::BIGINT AS restart_lsn_lag_bytes
  , PG_WAL_LSN_DIFF(PG_CURRENT_WAL_LSN(), confirmed_flush_lsn)::BIGINT AS confirmed_flush_lsn_lag_bytes
  , NULL::BIGINT AS inactive_seconds

FROM pg_replication_slots
ORDER BY
  CASE
    WHEN NOT active THEN 1
    WHEN wal_status = 'lost' THEN 2
    WHEN wal_status = 'unreserved' THEN 3
    ELSE 4
  END
  , restart_lsn_lag_bytes DESC NULLS LAST
`

type ReplicationSlotsPG15Row struct {
	SlotName                  pgtype.Text
	SlotType                  pgtype.Text
	Plugin                    pgtype.Text
	Database                  pgtype.Text
	Active                    pgtype.Bool
	ActivePid                 pgtype.Int4
	WalStatus                 pgtype.Text
	SafeWalSize               pgtype.Int8
	Temporary                 pgtype.Bool
	Conflicting               pgtype.Bool
	InvalidationReason        pgtype.Text
	RestartLsnLagBytes        pgtype.Int8
	ConfirmedFlushLsnLagBytes pgtype.Int8
	InactiveSeconds           pgtype.Int8
}

// For PostgreSQL 15/16: columns conflicting, invalidation_reason, inactive_since don't exist
func (q *Queries) ReplicationSlotsPG15(ctx context.Context) ([]ReplicationSlotsPG15Row, error) {
	rows, err := q.db.Query(ctx, replicationSlotsPG15)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ReplicationSlotsPG15Row
	for rows.Next() {
		var i ReplicationSlotsPG15Row
		if err := rows.Scan(
			&i.SlotName,
			&i.SlotType,
			&i.Plugin,
			&i.Database,
			&i.Active,
			&i.ActivePid,
			&i.WalStatus,
			&i.SafeWalSize,
			&i.Temporary,
			&i.Conflicting,
			&i.InvalidationReason,
			&i.RestartLsnLagBytes,
			&i.ConfirmedFlushLsnLagBytes,
			&i.InactiveSeconds,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const sequenceHealth = `-- name: SequenceHealth :many
WITH sequence_info AS (
  SELECT
    s.schemaname::text AS schema_name
    , s.sequencename::text AS sequence_name
    , s.data_type::text AS seq_data_type
    , s.max_value
    , s.increment_by
    , s.cycle AS is_cyclic
    , COALESCE(s.last_value, s.start_value) AS current_value
    , CASE
      WHEN s.max_value > 0 AND COALESCE(s.last_value, s.start_value) > 0
        THEN (COALESCE(s.last_value, s.start_value)::numeric / s.max_value::numeric) * 100
      ELSE 0
    END AS usage_percent
    , (s.max_value - COALESCE(s.last_value, s.start_value)) / NULLIF(s.increment_by, 0) AS remaining_values
  FROM pg_sequences AS s
  WHERE s.schemaname NOT IN ('pg_catalog', 'information_schema')
)

, sequence_owners AS (
  SELECT
    seq_ns.nspname::text AS seq_schema
    , seq_class.relname::text AS seq_name
    , tbl_ns.nspname::text AS table_schema
    , tbl_class.relname::text AS table_name
    , tbl_class.oid AS table_oid
    , attr.attname::text AS column_name
    , attr.attnum AS column_num
    , FORMAT_TYPE(attr.atttypid, attr.atttypmod)::text AS column_type
    , CASE FORMAT_TYPE(attr.atttypid, attr.atttypmod)
      WHEN 'integer' THEN 2147483647::bigint
      WHEN 'smallint' THEN 32767::bigint
      WHEN 'bigint' THEN 9223372036854775807::bigint
    END AS column_max_value
  FROM pg_depend AS dep
  INNER JOIN pg_class AS seq_class ON dep.objid = seq_class.oid AND seq_class.relkind = 'S'
  INNER JOIN pg_namespace AS seq_ns ON seq_class.relnamespace = seq_ns.oid
  INNER JOIN pg_class AS tbl_class ON dep.refobjid = tbl_class.oid AND tbl_class.relkind = 'r'
  INNER JOIN pg_namespace AS tbl_ns ON tbl_class.relnamespace = tbl_ns.oid
  INNER JOIN pg_attribute AS attr ON tbl_class.oid = attr.attrelid AND dep.refobjsubid = attr.attnum
  WHERE (
    dep.deptype = 'a'  -- Auto dependency (SERIAL creates this)
    OR attr.attidentity IN ('a', 'd')
  )  -- IDENTITY columns (PostgreSQL 10+)
  AND seq_ns.nspname NOT IN ('pg_catalog', 'information_schema')
)

, primary_keys AS (
  SELECT
    con.conrelid AS table_oid
    , UNNEST(con.conkey) AS column_num
  FROM pg_constraint AS con
  WHERE con.contype = 'p'  -- Primary key
)

, fk_references AS (
  SELECT
    con.confrelid AS referenced_table_oid
    , UNNEST(con.confkey) AS referenced_column_num
    , COUNT(*) AS fk_count
  FROM pg_constraint AS con
  WHERE con.contype = 'f'  -- Foreign key
  GROUP BY con.confrelid, UNNEST(con.confkey)
)

SELECT
  si.schema_name
  , si.sequence_name
  , si.seq_data_type
  , si.current_value
  , si.max_value
  , si.increment_by
  , si.is_cyclic
  , si.remaining_values
  , ROUND(si.usage_percent::numeric, 2) AS usage_percent
  , COALESCE(so.table_name, '') AS table_name
  , COALESCE(so.column_name, '') AS column_name
  , COALESCE(so.column_type, '') AS column_type
  , COALESCE(so.column_max_value, 0) AS column_max_value
  -- Flag if sequence can generate values that exceed column type
  , (so.column_max_value IS NOT NULL AND si.max_value > so.column_max_value) AS sequence_exceeds_column
  -- Flag if this is an integer column that should probably be bigint
  , (so.column_type != 'bigint' AND si.usage_percent > 50) AS should_be_bigint
  -- Flag if column is a primary key
  , (pk.table_oid IS NOT NULL) AS is_primary_key
  -- Count of foreign keys referencing this column
  , COALESCE(fkr.fk_count, 0) AS fk_reference_count

FROM sequence_info AS si
LEFT JOIN sequence_owners AS so
  ON
    si.schema_name = so.seq_schema
    AND si.sequence_name = so.seq_name
LEFT JOIN primary_keys AS pk
  ON
    so.table_oid = pk.table_oid
    AND so.column_num = pk.column_num
LEFT JOIN fk_references AS fkr
  ON
    so.table_oid = fkr.referenced_table_oid
    AND so.column_num = fkr.referenced_column_num
ORDER BY si.usage_percent DESC, si.remaining_values ASC
`

type SequenceHealthRow struct {
	SchemaName            pgtype.Text
	SequenceName          pgtype.Text
	SeqDataType           pgtype.Text
	CurrentValue          pgtype.Int8
	MaxValue              pgtype.Int8
	IncrementBy           pgtype.Int8
	IsCyclic              pgtype.Bool
	RemainingValues       pgtype.Int8
	UsagePercent          pgtype.Numeric
	TableName             pgtype.Text
	ColumnName            pgtype.Text
	ColumnType            pgtype.Text
	ColumnMaxValue        pgtype.Int8
	SequenceExceedsColumn pgtype.Bool
	ShouldBeBigint        pgtype.Bool
	IsPrimaryKey          pgtype.Bool
	FkReferenceCount      pgtype.Int8
}

// Identifies sequences approaching their maximum values and integer columns that should be bigint
// Find columns that own sequences (SERIAL/BIGSERIAL columns)
// Check if columns are primary keys
// Count foreign keys referencing each column (as the referenced/target column)
func (q *Queries) SequenceHealth(ctx context.Context) ([]SequenceHealthRow, error) {
	rows, err := q.db.Query(ctx, sequenceHealth)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []SequenceHealthRow
	for rows.Next() {
		var i SequenceHealthRow
		if err := rows.Scan(
			&i.SchemaName,
			&i.SequenceName,
			&i.SeqDataType,
			&i.CurrentValue,
			&i.MaxValue,
			&i.IncrementBy,
			&i.IsCyclic,
			&i.RemainingValues,
			&i.UsagePercent,
			&i.TableName,
			&i.ColumnName,
			&i.ColumnType,
			&i.ColumnMaxValue,
			&i.SequenceExceedsColumn,
			&i.ShouldBeBigint,
			&i.IsPrimaryKey,
			&i.FkReferenceCount,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const sessionSettings = `-- name: SessionSettings :many
/*
 * PostgreSQL settings follow a precedence hierarchy:
 * 1. System defaults (postgresql.conf)
 * 2. Database-level overrides (ALTER DATABASE ... SET)
 * 3. Role-level overrides (ALTER ROLE ... SET)  <- This query checks these
 * 4. Session-level changes (SET command)
 *
 * Technical approach:
 * - pg_db_role_setting stores role configs as text arrays: ['key=value', ...]
 * - UNNEST + split_part parse these into usable key/value pairs
 * - CROSS JOIN creates full matrix of roles × settings (shows gaps)
 * - LEFT JOIN preserves NULLs to identify which settings use defaults
 * - setdatabase = 0 filters for cluster-wide settings (not DB-specific)
 *
 * NOTE: reset_val represents the effective default (includes DB-level overrides),
 * not the raw postgresql.conf value. This is appropriate since we want to know
 * what the role actually gets vs what they override.
 */
WITH roles AS (
  SELECT
    r.rolname
    , r.oid
  FROM pg_roles AS r
  WHERE r.rolname IN (
    'app_ro'
    , 'app_rw'
  )
)

, settings AS (
  SELECT
    s.name
    , s.reset_val
    , s.unit
  FROM pg_settings AS s
  WHERE s.name IN (
    'statement_timeout'
    , 'idle_in_transaction_session_timeout'
    , 'transaction_timeout'
    , 'log_min_duration_statement'
  )
)

, role_configs AS (
  SELECT
    r.rolname
    , unnest(coalesce(
      (
        SELECT drs.setconfig
        FROM pg_db_role_setting AS drs
        WHERE
          drs.setrole = r.oid
          AND drs.setdatabase = 0
      )
      , ARRAY[]::text []
    )) AS config_setting
  FROM roles AS r
)

, parsed_configs AS (
  SELECT
    rolname
    , split_part(config_setting, '=', 1) AS param_name
    , split_part(config_setting, '=', 2) AS param_value
  FROM role_configs
)

SELECT
  r.rolname::varchar AS role_name
  , s.name::varchar AS setting_name
  , s.reset_val AS system_default
  , s.unit
  , coalesce(pc.param_value, s.reset_val) AS setting_value
  , CASE
    WHEN pc.param_value IS NOT NULL THEN 'OVERRIDE'
    ELSE 'DEFAULT'
  END AS status
FROM roles AS r
CROSS JOIN settings AS s
LEFT JOIN parsed_configs AS pc
  ON
    r.rolname = pc.rolname
    AND s.name = pc.param_name
ORDER BY r.rolname, s.name
`

type SessionSettingsRow struct {
	RoleName      pgtype.Text
	SettingName   pgtype.Text
	SystemDefault pgtype.Text
	Unit          pgtype.Text
	SettingValue  pgtype.Text
	Status        pgtype.Text
}

func (q *Queries) SessionSettings(ctx context.Context) ([]SessionSettingsRow, error) {
	rows, err := q.db.Query(ctx, sessionSettings)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []SessionSettingsRow
	for rows.Next() {
		var i SessionSettingsRow
		if err := rows.Scan(
			&i.RoleName,
			&i.SettingName,
			&i.SystemDefault,
			&i.Unit,
			&i.SettingValue,
			&i.Status,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const sessionStatistics = `-- name: SessionStatistics :one
SELECT
  COALESCE(SUM(session_time), 0)::double precision AS total_session_time_ms
  , COALESCE(SUM(active_time), 0)::double precision AS total_active_time_ms
  , COALESCE(SUM(idle_in_transaction_time), 0)::double precision AS total_idle_in_txn_time_ms
  , COALESCE(SUM(sessions), 0)::bigint AS total_sessions
  , COALESCE(SUM(sessions_abandoned), 0)::bigint AS sessions_abandoned
  , COALESCE(SUM(sessions_fatal), 0)::bigint AS sessions_fatal
  , COALESCE(SUM(sessions_killed), 0)::bigint AS sessions_killed
  -- Calculate session busy ratio (active_time / session_time)
  , CASE
    WHEN COALESCE(SUM(session_time), 0) > 0
      THEN ROUND((COALESCE(SUM(active_time), 0) / COALESCE(SUM(session_time), 0) * 100)::numeric, 2)
    ELSE 0
  END::double precision AS session_busy_ratio_percent
FROM pg_stat_database
WHERE
  datname IS NOT NULL
  AND datname NOT IN ('template0', 'template1')
`

type SessionStatisticsRow struct {
	TotalSessionTimeMs      pgtype.Float8
	TotalActiveTimeMs       pgtype.Float8
	TotalIdleInTxnTimeMs    pgtype.Float8
	TotalSessions           pgtype.Int8
	SessionsAbandoned       pgtype.Int8
	SessionsFatal           pgtype.Int8
	SessionsKilled          pgtype.Int8
	SessionBusyRatioPercent pgtype.Float8
}

// Gets session time statistics from pg_stat_database (PostgreSQL 14+).
// These stats help analyze connection pool efficiency.
// Returns zero values for PostgreSQL versions < 14 (columns don't exist).
func (q *Queries) SessionStatistics(ctx context.Context) (SessionStatisticsRow, error) {
	row := q.db.QueryRow(ctx, sessionStatistics)
	var i SessionStatisticsRow
	err := row.Scan(
		&i.TotalSessionTimeMs,
		&i.TotalActiveTimeMs,
		&i.TotalIdleInTxnTimeMs,
		&i.TotalSessions,
		&i.SessionsAbandoned,
		&i.SessionsFatal,
		&i.SessionsKilled,
		&i.SessionBusyRatioPercent,
	)
	return i, err
}

const statisticsFreshness = `-- name: StatisticsFreshness :one
SELECT
  stats_reset
  , coalesce(
    extract(EPOCH FROM (now() - stats_reset)) / 86400
    , 999
  )::int AS age_days
  , (now() - stats_reset) AS age_interval
FROM pg_stat_database
WHERE datname = current_database()
`

type StatisticsFreshnessRow struct {
	StatsReset  pgtype.Timestamptz
	AgeDays     pgtype.Int4
	AgeInterval pgtype.Interval
}

// Returns statistics age for the current database.
// Use to validate stats are meaningful before relying on usage-based checks.
func (q *Queries) StatisticsFreshness(ctx context.Context) (StatisticsFreshnessRow, error) {
	row := q.db.QueryRow(ctx, statisticsFreshness)
	var i StatisticsFreshnessRow
	err := row.Scan(&i.StatsReset, &i.AgeDays, &i.AgeInterval)
	return i, err
}

const tableActivity = `-- name: TableActivity :many
SELECT
  schemaname
  , relname
  , n_tup_ins
  , n_tup_upd
  , n_tup_del
  , n_tup_hot_upd
  , n_live_tup
  , pg_table_size(relid) AS table_size_bytes
FROM pg_stat_user_tables
WHERE n_tup_ins + n_tup_upd + n_tup_del > 0
ORDER BY n_tup_ins + n_tup_upd + n_tup_del DESC
`

type TableActivityRow struct {
	Schemaname     pgtype.Text
	Relname        pgtype.Text
	NTupIns        pgtype.Int8
	NTupUpd        pgtype.Int8
	NTupDel        pgtype.Int8
	NTupHotUpd     pgtype.Int8
	NLiveTup       pgtype.Int8
	TableSizeBytes pgtype.Int8
}

// Retrieves table write activity metrics from pg_stat_user_tables
// Used to identify high-churn tables and HOT update efficiency issues
func (q *Queries) TableActivity(ctx context.Context) ([]TableActivityRow, error) {
	rows, err := q.db.Query(ctx, tableActivity)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []TableActivityRow
	for rows.Next() {
		var i TableActivityRow
		if err := rows.Scan(
			&i.Schemaname,
			&i.Relname,
			&i.NTupIns,
			&i.NTupUpd,
			&i.NTupDel,
			&i.NTupHotUpd,
			&i.NLiveTup,
			&i.TableSizeBytes,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tableBloat = `-- name: TableBloat :many
SELECT
  (schemaname || '.' || relname)::text AS table_name
  , n_live_tup AS live_tuples
  , n_dead_tup AS dead_tuples
  , last_autovacuum
  , last_vacuum
  , last_autoanalyze
  , last_analyze
  , autovacuum_count
  , vacuum_count
  , n_mod_since_analyze AS modifications_since_analyze
  , CASE
    WHEN n_live_tup + n_dead_tup > 0
      THEN ROUND((n_dead_tup::numeric / (n_live_tup + n_dead_tup)::numeric) * 100, 2)
    ELSE 0
  END AS dead_tuple_percent
  , PG_TOTAL_RELATION_SIZE(schemaname || '.' || relname) AS total_size_bytes
FROM pg_stat_user_tables
WHERE
  schemaname NOT IN ('pg_catalog', 'information_schema')
  AND n_dead_tup > 1000  -- Ignore tiny tables with few dead tuples
ORDER BY dead_tuple_percent DESC, n_dead_tup DESC
`

type TableBloatRow struct {
	TableName                 pgtype.Text
	LiveTuples                pgtype.Int8
	DeadTuples                pgtype.Int8
	LastAutovacuum            pgtype.Timestamptz
	LastVacuum                pgtype.Timestamptz
	LastAutoanalyze           pgtype.Timestamptz
	LastAnalyze               pgtype.Timestamptz
	AutovacuumCount           pgtype.Int8
	VacuumCount               pgtype.Int8
	ModificationsSinceAnalyze pgtype.Int8
	DeadTuplePercent          pgtype.Numeric
	TotalSizeBytes            pgtype.Int8
}

// Identifies tables with high dead tuple percentages indicating vacuum issues
func (q *Queries) TableBloat(ctx context.Context) ([]TableBloatRow, error) {
	rows, err := q.db.Query(ctx, tableBloat)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []TableBloatRow
	for rows.Next() {
		var i TableBloatRow
		if err := rows.Scan(
			&i.TableName,
			&i.LiveTuples,
			&i.DeadTuples,
			&i.LastAutovacuum,
			&i.LastVacuum,
			&i.LastAutoanalyze,
			&i.LastAnalyze,
			&i.AutovacuumCount,
			&i.VacuumCount,
			&i.ModificationsSinceAnalyze,
			&i.DeadTuplePercent,
			&i.TotalSizeBytes,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tableFreezeAge = `-- name: TableFreezeAge :many
SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , c.relfrozenxid::text AS frozen_xid
  , s.last_autovacuum
  , s.last_vacuum
  , s.autovacuum_count
  , s.vacuum_count
  , age(c.relfrozenxid) AS freeze_age
  , pg_total_relation_size(c.oid) AS table_size_bytes
FROM pg_class AS c
INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
WHERE
  c.relkind = 'r'
  AND n.nspname = 'public'
  AND c.relfrozenxid != '0'
ORDER BY age(c.relfrozenxid) DESC
LIMIT 50
`

type TableFreezeAgeRow struct {
	TableName       pgtype.Text
	FrozenXid       pgtype.Text
	LastAutovacuum  pgtype.Timestamptz
	LastVacuum      pgtype.Timestamptz
	AutovacuumCount pgtype.Int8
	VacuumCount     pgtype.Int8
	FreezeAge       pgtype.Int4
	TableSizeBytes  pgtype.Int8
}

// Gets transaction ID age for tables with oldest frozen XIDs.
func (q *Queries) TableFreezeAge(ctx context.Context) ([]TableFreezeAgeRow, error) {
	rows, err := q.db.Query(ctx, tableFreezeAge)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []TableFreezeAgeRow
	for rows.Next() {
		var i TableFreezeAgeRow
		if err := rows.Scan(
			&i.TableName,
			&i.FrozenXid,
			&i.LastAutovacuum,
			&i.LastVacuum,
			&i.AutovacuumCount,
			&i.VacuumCount,
			&i.FreezeAge,
			&i.TableSizeBytes,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tableVacuumHealth = `-- name: TableVacuumHealth :many
SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , s.last_autovacuum
  , COALESCE(s.n_live_tup, c.reltuples::bigint) AS estimated_rows
  , PG_TOTAL_RELATION_SIZE(c.oid) AS table_size_bytes
  , COALESCE(s.n_dead_tup, 0) AS n_dead_tup
  , COALESCE(s.autovacuum_count, 0) AS autovacuum_count
  , ARRAY_TO_STRING(c.reloptions, ',') AS reloptions
  , GREATEST(s.last_vacuum, s.last_autovacuum) AS last_vacuum_any
  , GREATEST(s.last_analyze, s.last_autoanalyze) AS last_analyze_any
  -- Stats staleness indicators
  , COALESCE(s.n_mod_since_analyze, 0) AS n_mod_since_analyze
  , COALESCE(s.autoanalyze_count, 0) AS autoanalyze_count
  -- PG14+ columns for insert tracking (will be 0 on older versions via COALESCE)
  , COALESCE(s.n_ins_since_vacuum, 0) AS n_ins_since_vacuum
FROM pg_class AS c
INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
LEFT JOIN pg_stat_user_tables AS s ON c.oid = s.relid
WHERE
  c.relkind IN ('r', 'p')
  AND n.nspname = 'public'
ORDER BY COALESCE(s.n_live_tup, c.reltuples::bigint) DESC
`

type TableVacuumHealthRow struct {
	TableName        pgtype.Text
	LastAutovacuum   pgtype.Timestamptz
	EstimatedRows    pgtype.Int8
	TableSizeBytes   pgtype.Int8
	NDeadTup         pgtype.Int8
	AutovacuumCount  pgtype.Int8
	Reloptions       pgtype.Text
	LastVacuumAny    pgtype.Timestamptz
	LastAnalyzeAny   pgtype.Timestamptz
	NModSinceAnalyze pgtype.Int8
	AutoanalyzeCount pgtype.Int8
	NInsSinceVacuum  pgtype.Int8
}

// Returns all tables with vacuum-related health metrics.
// Used by multiple subchecks: autovacuum-disabled, large-table-defaults, vacuum-stale, analyze-needed.
func (q *Queries) TableVacuumHealth(ctx context.Context) ([]TableVacuumHealthRow, error) {
	rows, err := q.db.Query(ctx, tableVacuumHealth)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []TableVacuumHealthRow
	for rows.Next() {
		var i TableVacuumHealthRow
		if err := rows.Scan(
			&i.TableName,
			&i.LastAutovacuum,
			&i.EstimatedRows,
			&i.TableSizeBytes,
			&i.NDeadTup,
			&i.AutovacuumCount,
			&i.Reloptions,
			&i.LastVacuumAny,
			&i.LastAnalyzeAny,
			&i.NModSinceAnalyze,
			&i.AutoanalyzeCount,
			&i.NInsSinceVacuum,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const tempUsage = `-- name: TempUsage :one
WITH temp_stats AS (
  SELECT
    datname::text AS database_name
    , temp_files
    , temp_bytes
    , stats_reset
    , EXTRACT(EPOCH FROM (NOW() - stats_reset)) AS seconds_since_reset
    , CASE
      WHEN EXTRACT(EPOCH FROM (NOW() - stats_reset)) > 0
        THEN temp_files::numeric / (EXTRACT(EPOCH FROM (NOW() - stats_reset)) / 3600)
      ELSE 0
    END AS temp_files_per_hour
    , CASE
      WHEN EXTRACT(EPOCH FROM (NOW() - stats_reset)) > 0
        THEN temp_bytes::numeric / (EXTRACT(EPOCH FROM (NOW() - stats_reset)) / 3600)
      ELSE 0
    END AS temp_bytes_per_hour
  FROM pg_stat_database
  WHERE datname = CURRENT_DATABASE()
)

, memory_settings AS (
  SELECT
    (
      SELECT setting FROM pg_settings
      WHERE name = 'work_mem'
    ) AS work_mem
    , (
      SELECT setting FROM pg_settings
      WHERE name = 'temp_file_limit'
    ) AS temp_file_limit
    , (
      SELECT setting FROM pg_settings
      WHERE name = 'log_temp_files'
    ) AS log_temp_files
    , (
      SELECT setting FROM pg_settings
      WHERE name = 'max_connections'
    ) AS max_connections
    , (
      SELECT setting FROM pg_settings
      WHERE name = 'shared_buffers'
    ) AS shared_buffers
)

SELECT
  ts.database_name
  , ts.temp_files
  , ts.temp_bytes
  , ts.stats_reset
  , ts.seconds_since_reset
  , ms.work_mem
  , ms.temp_file_limit
  , ms.log_temp_files
  , ms.max_connections
  , ms.shared_buffers
  , ROUND(ts.temp_files_per_hour::numeric, 2) AS temp_files_per_hour
  , ROUND(ts.temp_bytes_per_hour::numeric, 0) AS temp_bytes_per_hour
FROM temp_stats AS ts
CROSS JOIN memory_settings AS ms
`

type TempUsageRow struct {
	DatabaseName      pgtype.Text
	TempFiles         pgtype.Int8
	TempBytes         pgtype.Int8
	StatsReset        pgtype.Timestamptz
	SecondsSinceReset pgtype.Numeric
	WorkMem           pgtype.Text
	TempFileLimit     pgtype.Text
	LogTempFiles      pgtype.Text
	MaxConnections    pgtype.Text
	SharedBuffers     pgtype.Text
	TempFilesPerHour  pgtype.Numeric
	TempBytesPerHour  pgtype.Numeric
}

// Monitors temporary file creation indicating work_mem exhaustion
func (q *Queries) TempUsage(ctx context.Context) (TempUsageRow, error) {
	row := q.db.QueryRow(ctx, tempUsage)
	var i TempUsageRow
	err := row.Scan(
		&i.DatabaseName,
		&i.TempFiles,
		&i.TempBytes,
		&i.StatsReset,
		&i.SecondsSinceReset,
		&i.WorkMem,
		&i.TempFileLimit,
		&i.LogTempFiles,
		&i.MaxConnections,
		&i.SharedBuffers,
		&i.TempFilesPerHour,
		&i.TempBytesPerHour,
	)
	return i, err
}

const toastStorage = `-- name: ToastStorage :many
WITH toast_info AS (
  SELECT
    n.nspname::text AS schema_name
    , c.relname::text AS table_name
    , t.relname::text AS toast_table_name
    , pg_relation_size(c.oid) AS main_table_size
    , pg_relation_size(t.oid) AS toast_size
    , pg_total_relation_size(c.oid) AS total_size
    , pg_indexes_size(c.oid) AS indexes_size
    , CASE
      WHEN pg_total_relation_size(c.oid) > 0
        THEN round((pg_relation_size(t.oid)::numeric / pg_total_relation_size(c.oid)::numeric) * 100, 2)
      ELSE 0
    END AS toast_percent
    -- TOAST table statistics for bloat detection
    , coalesce(st.n_live_tup, 0) AS toast_live_tuples
    , coalesce(st.n_dead_tup, 0) AS toast_dead_tuples
  FROM pg_class AS c
  INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
  INNER JOIN pg_class AS t ON c.reltoastrelid = t.oid
  LEFT JOIN pg_stat_user_tables AS st ON t.oid = st.relid
  WHERE
    c.relkind IN ('r', 'p')
    AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
    AND c.reltoastrelid != 0
    AND pg_relation_size(t.oid) > 1048576  -- TOAST > 1MB
)

, wide_columns AS (
  SELECT
    ps.schemaname::text AS schema_name
    , ps.tablename::text AS table_name
    , ps.attname::text AS column_name
    , ps.avg_width
    , CASE
      WHEN pt.typname IN ('json', 'jsonb') THEN 'jsonb'
      WHEN pt.typname IN ('text', 'varchar', 'char', 'bpchar') THEN 'text'
      WHEN pt.typname = 'bytea' THEN 'bytea'
      ELSE 'other'
    END AS column_category
  FROM pg_stats AS ps
  INNER JOIN pg_class AS c ON ps.tablename = c.relname
  INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid AND ps.schemaname = n.nspname
  INNER JOIN pg_attribute AS pa ON c.oid = pa.attrelid AND ps.attname = pa.attname
  INNER JOIN pg_type AS pt ON pa.atttypid = pt.oid
  WHERE
    ps.schemaname NOT IN ('pg_catalog', 'information_schema')
    AND ps.avg_width > 2000  -- Likely using TOAST (threshold ~2KB)
    AND ps.avg_width IS NOT NULL
)

, column_compression AS (
  SELECT
    n.nspname::text AS schema_name
    , c.relname::text AS table_name
    , a.attname::text AS column_name
    , t.typname::text AS column_type
    , CASE a.attcompression
      WHEN 'p' THEN 'pglz'
      WHEN 'l' THEN 'lz4'
      ELSE 'default'
    END AS compression_algorithm
    , CASE a.attstorage
      WHEN 'p' THEN 'PLAIN'
      WHEN 'e' THEN 'EXTERNAL'
      WHEN 'x' THEN 'EXTENDED'
      WHEN 'm' THEN 'MAIN'
    END AS storage_strategy
  FROM pg_attribute AS a
  INNER JOIN pg_class AS c ON a.attrelid = c.oid
  INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
  INNER JOIN pg_type AS t ON a.atttypid = t.oid
  WHERE
    a.attnum > 0  -- Exclude system columns
    AND NOT a.attisdropped
    AND a.attstorage IN ('x', 'e', 'm')  -- Columns that can use TOAST
    AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
    AND c.relkind IN ('r', 'p')  -- Regular and partitioned tables
    AND t.typname IN ('text', 'varchar', 'bpchar', 'json', 'jsonb', 'bytea')  -- TOAST-able types
)

SELECT
  ti.schema_name
  , ti.table_name
  , ti.toast_table_name
  , ti.main_table_size
  , ti.toast_size
  , ti.total_size
  , ti.indexes_size
  , ti.toast_percent
  , ti.toast_live_tuples
  , ti.toast_dead_tuples
  , coalesce(
    (
      SELECT array_agg(wc.column_name || ':' || wc.avg_width::text || ':' || wc.column_category ORDER BY wc.avg_width DESC)
      FROM wide_columns AS wc
      WHERE wc.schema_name = ti.schema_name AND wc.table_name = ti.table_name
    )
    , ARRAY[]::text []
  ) AS wide_columns
  , coalesce(
    (
      SELECT
        array_agg(
          cc.column_name || ':' || cc.compression_algorithm || ':' || cc.storage_strategy || ':' || cc.column_type
          ORDER BY cc.column_name
        )
      FROM column_compression AS cc
      WHERE cc.schema_name = ti.schema_name AND cc.table_name = ti.table_name
    )
    , ARRAY[]::text []
  ) AS column_compression_info
FROM toast_info AS ti
ORDER BY ti.toast_size DESC
`

type ToastStorageRow struct {
	SchemaName            pgtype.Text
	TableName             pgtype.Text
	ToastTableName        pgtype.Text
	MainTableSize         pgtype.Int8
	ToastSize             pgtype.Int8
	TotalSize             pgtype.Int8
	IndexesSize           pgtype.Int8
	ToastPercent          pgtype.Numeric
	ToastLiveTuples       pgtype.Int8
	ToastDeadTuples       pgtype.Int8
	WideColumns           []string
	ColumnCompressionInfo []string
}

// Analyzes TOAST storage usage and identifies tables with large value storage
func (q *Queries) ToastStorage(ctx context.Context) ([]ToastStorageRow, error) {
	rows, err := q.db.Query(ctx, toastStorage)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ToastStorageRow
	for rows.Next() {
		var i ToastStorageRow
		if err := rows.Scan(
			&i.SchemaName,
			&i.TableName,
			&i.ToastTableName,
			&i.MainTableSize,
			&i.ToastSize,
			&i.TotalSize,
			&i.IndexesSize,
			&i.ToastPercent,
			&i.ToastLiveTuples,
			&i.ToastDeadTuples,
			&i.WideColumns,
			&i.ColumnCompressionInfo,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const uuidColumnDefaults = `-- name: UuidColumnDefaults :many
WITH indexed_columns AS (
  SELECT
    i.indrelid AS table_oid
    , unnest(i.indkey) AS column_num
  FROM pg_index AS i
)

SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , a.attname::text AS column_name
  , pg_get_expr(d.adbin, d.adrelid)::text AS default_expr
  , (idx.column_num IS NOT NULL) AS has_index
FROM pg_attribute AS a
INNER JOIN pg_class AS c ON a.attrelid = c.oid
INNER JOIN pg_namespace AS n ON c.relnamespace = n.oid
INNER JOIN pg_type AS t ON a.atttypid = t.oid
LEFT JOIN pg_attrdef AS d ON c.oid = d.adrelid AND a.attnum = d.adnum
LEFT JOIN indexed_columns AS idx
  ON c.oid = idx.table_oid AND a.attnum = idx.column_num
WHERE
  a.attnum > 0
  AND NOT a.attisdropped
  AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
  AND c.relkind IN ('r', 'p')
  AND t.typname = 'uuid'
  AND d.adbin IS NOT NULL
`

type UuidColumnDefaultsRow struct {
	TableName   pgtype.Text
	ColumnName  pgtype.Text
	DefaultExpr pgtype.Text
	HasIndex    pgtype.Bool
}

// Find UUID columns with their DEFAULT expressions to detect random UUID usage.
func (q *Queries) UuidColumnDefaults(ctx context.Context) ([]UuidColumnDefaultsRow, error) {
	rows, err := q.db.Query(ctx, uuidColumnDefaults)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []UuidColumnDefaultsRow
	for rows.Next() {
		var i UuidColumnDefaultsRow
		if err := rows.Scan(
			&i.TableName,
			&i.ColumnName,
			&i.DefaultExpr,
			&i.HasIndex,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const uuidColumnsAsString = `-- name: UuidColumnsAsString :many
SELECT
  (n.nspname || '.' || c.relname)::text AS table_name
  , a.attname::text AS column_name
  , t.typname::text AS column_type
  , pg_catalog.pg_table_size(c.oid) AS table_size_bytes
FROM pg_catalog.pg_attribute AS a
INNER JOIN pg_catalog.pg_class AS c ON a.attrelid = c.oid
INNER JOIN pg_catalog.pg_namespace AS n ON c.relnamespace = n.oid
INNER JOIN pg_catalog.pg_type AS t ON a.atttypid = t.oid
WHERE
  a.attnum > 0
  AND NOT a.attisdropped
  AND n.nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
  AND c.relkind IN ('r', 'p')
  AND t.typname IN ('varchar', 'text', 'bpchar', 'char')
  AND a.attname ~* 'uuid'
ORDER BY pg_catalog.pg_table_size(c.oid) DESC
`

type UuidColumnsAsStringRow struct {
	TableName      string
	ColumnName     string
	ColumnType     string
	TableSizeBytes int64
}

// Identifies columns that appear to store UUIDs but use string types.
func (q *Queries) UuidColumnsAsString(ctx context.Context) ([]UuidColumnsAsStringRow, error) {
	rows, err := q.db.Query(ctx, uuidColumnsAsString)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []UuidColumnsAsStringRow
	for rows.Next() {
		var i UuidColumnsAsStringRow
		if err := rows.Scan(
			&i.TableName,
			&i.ColumnName,
			&i.ColumnType,
			&i.TableSizeBytes,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const vacuumSettings = `-- name: VacuumSettings :many
SELECT
  name::varchar
  , setting
  , unit
FROM pg_settings
WHERE
  name IN (
    'autovacuum_analyze_scale_factor'
    , 'autovacuum_max_workers'
    , 'autovacuum_vacuum_scale_factor'
    , 'maintenance_work_mem'
    , 'max_connections'
    , 'vacuum_cost_delay'
    , 'vacuum_cost_limit'
    , 'work_mem'
  )
UNION
SELECT
  'active_connections' AS name
  , COUNT(*)::varchar AS setting
  , NULL AS unit
FROM pg_stat_activity
`

type VacuumSettingsRow struct {
	Name    pgtype.Text
	Setting pgtype.Text
	Unit    pgtype.Text
}

// noqa: disable=RF04
func (q *Queries) VacuumSettings(ctx context.Context) ([]VacuumSettingsRow, error) {
	rows, err := q.db.Query(ctx, vacuumSettings)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []VacuumSettingsRow
	for rows.Next() {
		var i VacuumSettingsRow
		if err := rows.Scan(&i.Name, &i.Setting, &i.Unit); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
